# -*- coding: utf-8 -*-
"""Autism prediction using machine learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14L_04u--aYlm-6a7iZZukaryqFtop2cz
"""

!pip install optuna

!pip install optuna

import os
import numpy as np
import pandas as pd
import seaborn as sns
from pathlib import Path
import missingno as msno
import plotly
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split ,KFold,StratifiedKFold
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from xgboost import XGBClassifier
import lightgbm as lgb
from sklearn.metrics import roc_curve, auc ,roc_auc_score
import optuna # optuna library is now installed and can be imported
from sklearn.preprocessing import LabelEncoder
from math import factorial
from scipy.stats import mode
# Use 'seaborn-v0_8-whitegrid' instead of 'seaborn-whitegrid'
plt.style.use("seaborn-v0_8-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=16,
    titlepad=10,
)
import warnings
warnings.filterwarnings('ignore')
print("Packages Imported ")

"""**Data loading**"""

train = pd.read_csv('/content/train.csv')
test  = pd.read_csv('/content/test.csv')
sample_sub= pd.read_csv('/content/sample_submission.csv')

train.columns

train.head(5)

"""**General information about the train data**"""

train.info()
train.shape

"""**Rename some columns**"""

train= train.rename(columns = {'austim': 'autism', 'contry_of_res':'country_of_res','Class/ASD':'Class'})
test= test.rename(columns = {'austim': 'autism', 'contry_of_res':'country_of_res'})

"""**Basic summary statistic**

let's take a look at the summary statistics table ( information about Mean Max, Min and others )
"""

train.describe().style.background_gradient(cmap='BuPu_r')

"""Now are going to take a deep look at each variable in our train data and explore how each variable relates to presence of autism with in the patient.

Autism by Gender
The gender variable is ballanced in our train data , looking at the figure below we can see that autism is more present with in female then in males.
"""

plt.figure(figsize=(10,5))
sns.histplot(x= "gender", data=train,shrink=.8 ,hue = "gender");

plt.figure(figsize=(12,5))
sns.histplot(data=train, x="gender", hue="Class", multiple="dodge", palette ='flare',shrink=.8)
plt.title(' Counnts of Autism cases by gender')

"""**Autism by ethnicity**"""

ethnicity_plt = train["ethnicity"].value_counts().reset_index()
ethnicity_plt.columns = ["ethnicity", "count"]  # Rename columns for clarity
ethnicity_plt = ethnicity_plt.sort_values("count", ascending=False)  # Sort by count

plt.figure(figsize=(16, 6))
sns.barplot(x="ethnicity", y="count", data=ethnicity_plt, palette="BuGn_r")
plt.title("Distribution of Ethnicities", fontsize=16)
plt.xlabel("Ethnicity", fontsize=14)
plt.ylabel("Count", fontsize=14)
plt.xticks(rotation=45)
plt.show()

country_plt= train["country_of_res"].value_counts().reset_index()
plt.figure(figsize=(20,6))
sns.barplot(y="country_of_res",data=country_plt.sort_values("country_of_res",ascending=False).iloc[:10,:],palette="BuPu_r")
#####
plt.figure(figsize=(16,6))
country_plt=train.groupby(['country_of_res','Class']).size().reset_index(name='Size')
order=["United States","United Arab Emirates","New Zealand","India","United Kingdom","Australia","Jordan","Afghanistan","Sri Lanka","Canada"]
sns.barplot(data=country_plt,x="country_of_res",y="Size",hue='Class', palette='BuPu_r',order=order);
plt.title(' Autism by country of residence ')

fig, ax = plt.subplots(2, 1,figsize=(13,7))
sns.countplot(x="jaundice",data=train,palette="YlOrBr_r",ax=ax[0])
jaundic_plt=train.groupby(['jaundice','Class']).size().reset_index(name='Size')
sns.barplot(data=jaundic_plt,x="jaundice",y="Size",hue='Class', palette='YlOrBr_r',ax=ax[1])
plt.title(' Autism by jaundice ')

"""Autism in family"""

fig, ax = plt.subplots(2, 1,figsize=(12,7))
sns.countplot(x="autism",data=train,palette="rocket",ax=ax[0])
autusim_plt=train.groupby(['autism','Class']).size().reset_index(name='Size')
sns.barplot(data=autusim_plt,x="autism",y="Size",hue='Class', palette='rocket',ax=ax[1])
plt.title('autism presence in the family ')

"""**Autism by screening test**
used_app_before feature repersent Whether the patient has undergone a screening test before .
"""

fig, ax = plt.subplots(2, 1,figsize=(11,8))
sns.countplot(x="used_app_before",data=train,palette="Greys_r",ax=ax[0])
used_app_before_plt=train.groupby(['used_app_before','Class']).size().reset_index(name='Size')
sns.barplot(data=used_app_before_plt,x="used_app_before",y="Size",hue='Class', palette='Greys_r',ax=ax[1])
plt.title('Autism by screening test')

"""Looking at the above plots we can see that from 800 patients only 35 patients has undergone a screening test before. we can tell that autism presence is almost equal percentage in the two cases ( around 20 %).


Relation of patient who completed the test
"""

plt.figure(figsize=(12,5))
relation_plot=train.groupby(['relation','Class']).size().reset_index(name='Size')
sns.barplot(data=relation_plot.sort_values("Size",ascending=False),x="relation",y="Size",hue='Class', palette='PuRd_r')
plt.title("Relation of patient who completed the test")

"""Most patients take the test themselves (617) , followed by 'unknown' (77) , then parents (49).


Patients age Distribution
"""

print("Basic summary statistics for age column")
pd.DataFrame(train["age"].describe()).T

plt.figure(figsize=(13,6))
age_plot=train.groupby(['age','Class']).size().reset_index(name="size")
sns.histplot(data=age_plot,x='age',hue="Class" , kde=True , palette= 'Dark2');
#sns.rugplot(data=age_plot,x='age',hue="Class" , palette= 'brg')
plt.title("Relation of patient who completed the test")

"""**A_Score Feature**

A1_Score to A10_score is a Score based on Autism Spectrum Quotient (AQ)10 item screening tool. it's a binary value ( 0 or 1 ),
"""

A_feat = ['A1_Score','A6_Score', 'A2_Score', 'A7_Score','A3_Score', 'A8_Score','A4_Score','A9_Score', 'A5_Score','A10_Score',]
i = 1
plt.figure()
fig, ax = plt.subplots(figsize=(16, 13))
for col in A_feat:
    plt.subplot(5,2,i)
    sns.countplot(x=train[col],data=train,palette="flare_r")
    i += 1

"""Screening Test Result

the Result feature represent the Score for AQ1-10 screening test , it's a numirical feature
"""

sns.displot(data=train,x='result',hue="Class" ,kind="kde", palette = "CMRmap_r",fill=True,height=6, aspect=1.7) ;
#sns.rugplot(data=age_plot,x='age',hue="Class" , palette= 'brg')
plt.title("Screening Test Result distrubtion")
plt.show()

"""Feature variation


We want see how variant are our feature , that gives us idea which feature will be more important.
"""

# variables variation
df_var = train.select_dtypes(include=np.number).var().reset_index() # Select only numeric columns
df_var.columns = ['feature', 'variation']
df_var.sort_values("variation", ascending=False)

"""**Feature corelation**"""

# Correlation matrix
# Select only numeric features for correlation analysis
numeric_train = train.select_dtypes(include=np.number)

# Calculate the correlation matrix
corrMatrix = numeric_train.corr(method='pearson', min_periods=1)

# Apply background gradient for visualization
corrMatrix.style.background_gradient(axis=None)

##heatmap
plt.figure(figsize=(16,8))
ax = sns.heatmap(corrMatrix, cmap="YlGnBu",annot=True)

cor_targ = train.select_dtypes(include=np.number).corrwith(train["Class"]).reset_index() # Select only numeric features for corrwith
cor_targ.columns =['Numerical feature', 'CorrelatioWithTarget']
cor_targ.sort_values('CorrelatioWithTarget',ascending = False)

"""Look like features have close correlation values with the targt where A3 , A6 and A9 being most correlated to target.

**Trgat variable**

our target is binary variable , we are predicting the likelihood of having autism
"""

print('percentage of each class:')
percent_value = pd.DataFrame(train['Class'].value_counts()/len(train))
percent_value.T

countplt, ax = plt.subplots(figsize = (8,5))
ax =sns.countplot(train['Class'],palette="gist_heat")

"""As we can see that train dataset has negative cases more than positive cases . 0.76875% of the data are no atism cases and 0.23125 are indeed autism case, which make it a sort of imbalanced binarry classification.

After we take a look at our data and features, now are going to model it and take prediction for the test data , for that we are going to use Extra trees Classifier ,and we use Optuna to fine tune it's hyperparamaters. let's start.

**Data Preparation**
"""

X = train.drop(columns=["ID","Class",'used_app_before','result']).copy() #drop some usless features
y = train["Class"].copy()
X_test = test.drop(columns=["ID",'used_app_before','result']).copy()

cat_col = [col for col in X.columns if X[col].dtype == 'object']
num_col = [col for col in X.columns if X[col].dtype == 'int']

"""**Label Encoder for categorical data**"""

label_encoder = LabelEncoder()
train_x = X.copy()
test_x = X_test.copy()
for col in cat_col:
        train_x[col] = label_encoder.fit_transform(train_x[col])
        test_x[col] = label_encoder.fit_transform(test_x[col])

train_x.shape , test_x.shape , y.shape

"""**Optuna hyperparamateres optimization**"""

X_train, X_valid, y_train, y_valid = train_test_split(train_x,y ,test_size=0.15, random_state=42)
def objective(trial):
    n_estimators = trial.suggest_int("n_estimators",8,5000)
    max_depth = trial.suggest_int("max_depth", 4, 200)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 16)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 15)
    criterion = trial.suggest_categorical("criterion", ['gini', 'entropy'])
    clf = ExtraTreesClassifier(n_estimators = n_estimators,
                               max_depth = max_depth,
                               min_samples_split = min_samples_split,
                               min_samples_leaf = min_samples_leaf,
                               criterion = criterion,
                               random_state = 42,
                              )
    clf.fit(X_train, y_train)
    return clf.score(X_valid, y_valid)

"""Optimize the function and retreive best paramaters"""

study = optuna.create_study(direction = "maximize") ##creat stuudy
study.optimize(objective, n_trials =50) ## optimize study

parameters = study.best_params
parameters

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ## Best hyper param from optuna study
# n_estimators = 3647
# max_depth = 75
# min_samples_split = 14
# min_samples_leaf = 15
# criterion = 'gini'
# ##
# splits = 5 # number of folds
# predictions = [] # list of predictions
# scores = [] # auc_roc validation score
# feat_imp = pd.DataFrame() # get feat importance
# train_x = train_x.values ## convert Df to numpy array
# kf =  StratifiedKFold(n_splits=splits, shuffle=True, random_state=42) # creat folds
# ###training extra trees classifier ###
# for fold, (idx_train, idx_valid) in enumerate(kf.split(train_x,y)):
#     X_tr, y_tr = train_x[idx_train], y.iloc[idx_train]
#     X_val, y_val = train_x[idx_valid], y.iloc[idx_valid]
#     model =  ExtraTreesClassifier(n_estimators = n_estimators,
#                                  max_depth = max_depth,
#                                  min_samples_split = min_samples_split,
#                                  min_samples_leaf = min_samples_leaf,
#                                  criterion = criterion,
#                                  random_state = 42)
#     model.fit(X_tr,y_tr)
#     val_pred = model.predict_proba(X_val)[:, 1]
#     score = roc_auc_score(y_val, val_pred)
#     scores.append(score)
#     print(f"Fold: {fold + 1} roc_auc Score is : {score}")
#     fold_imp= pd.DataFrame()
#     fold_imp["Feature"] = test_x.columns
#     fold_imp["importance"] = model.feature_importances_
#     fold_imp["fold"] = fold+ 1
#     feat_imp = pd.concat([feat_imp, fold_imp], axis=0)
#     print('*'*40)
#     test_preds = model.predict_proba(test_x)[:, 1]
#     predictions.append(test_preds)
# print(f" mean Validation roc_aucis : {np.mean(scores)}")

"""**Feature importance**"""

plot =feat_imp.groupby("Feature").mean().reset_index()
plt.figure(figsize=(18,10))
sns.barplot(x="importance", y="Feature", data=plot.sort_values(by="importance", ascending=False),palette = 'YlGnBu_r')
plt.title('Extra_Tree Features importance ')
plt.tight_layout()
plt.show()

"""**Submission**"""

preds = np.array(predictions)
sample_sub["Class/ASD"] = preds.mean(axis=0) # take the mean from folds predic
sample_sub.to_csv("extra_tree_submission.csv",index=False)
sample_sub["Class/ASD"].hist(color ="darkblue")